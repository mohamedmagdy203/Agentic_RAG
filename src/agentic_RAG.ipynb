{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import PyPDF2\n",
    "import wikipedia\n",
    "import requests\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.agents import AgentExecutor, initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from tavily import TavilyClient\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936811ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key=os.getenv('GOOGLE_API_KEY')\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224321e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_pro = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gemini_pro=genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedAgenticRAG:\n",
    "    def __init__(self, db_directory=\"./chroma_db\"):\n",
    "        \"\"\"\n",
    "        Initialize the Advanced Agentic RAG system.\n",
    "        \n",
    "        Args:\n",
    "            db_directory (str): Path to store the vector database\n",
    "        \"\"\"\n",
    "        self.db_directory = db_directory\n",
    "        self.tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "        try:\n",
    "            self.vector_store = Chroma(\n",
    "                persist_directory=db_directory,\n",
    "                embedding_function=embeddings\n",
    "            )\n",
    "            print(f\"Vector database loaded from {db_directory}\")\n",
    "        except:\n",
    "            self.vector_store = Chroma(\n",
    "                embedding_function=embeddings,\n",
    "                persist_directory=db_directory\n",
    "            )\n",
    "            print(f\"New vector database created at {db_directory}\")\n",
    "\n",
    "        self.setup_tools()\n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_tools(self):\n",
    "        \"\"\"Define the tools the agent can use\"\"\"\n",
    "        self.tools = [\n",
    "            Tool(\n",
    "                name=\"SearchDatabase\",\n",
    "                func=self.search_vector_db,\n",
    "                description=\"Search the vector database for relevant information\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"ProcessPDF\",\n",
    "                func=self.process_pdf,\n",
    "                description=\"Extract text from a PDF file and store it in the vector database\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"WebSearch\",\n",
    "                func=self.web_search,\n",
    "                description=\"Perform web search using Tavily Search API to fetch recent information\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"ProcessWikipedia\",\n",
    "                func=self.process_wikipedia,\n",
    "                description=\"Retrieve content from a Wikipedia article and store it in the vector database\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"QueryRefiner\",\n",
    "                func=self.refine_query,\n",
    "                description=\"Improve a search query for better retrieval results\"\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def setup_agent(self):\n",
    "        \"\"\"Configure the agent using ReAct pattern\"\"\"\n",
    "        self.agent_executor = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=gemini_pro,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True,\n",
    "            handle_parsing_errors=True,\n",
    "            max_iterations=5\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text content from a PDF file\"\"\"\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting text from PDF: {str(e)}\"\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF and store it in the vector DB\"\"\"\n",
    "        try:\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            chunks = text_splitter.split_text(text)\n",
    "\n",
    "            from langchain_core.documents import Document\n",
    "            documents = [Document(page_content=chunk, metadata={\"source\": pdf_path, \"type\": \"pdf\"}) for chunk in chunks]\n",
    "\n",
    "            self.vector_store.add_documents(documents)\n",
    "            self.vector_store.persist()\n",
    "\n",
    "            return f\"{len(chunks)} text chunks from '{pdf_path}' loaded into the vector database.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error processing PDF: {str(e)}\"\n",
    "\n",
    "    def web_search(self, query: str) -> str:\n",
    "        \"\"\"Perform a web search using the Tavily Search API\"\"\"\n",
    "        try:\n",
    "            search_result = self.tavily_client.search(\n",
    "                query=query,\n",
    "                search_depth=\"advanced\",\n",
    "                include_answer=True,\n",
    "                include_domains=[]\n",
    "            )\n",
    "\n",
    "            result_text = search_result.get('answer', '') + \"\\n\\nSources:\\n\"\n",
    "            for i, result in enumerate(search_result.get('results', [])[:3]):\n",
    "                result_text += f\"{i+1}. {result.get('title')}: {result.get('url')}\\n\"\n",
    "                result_text += f\"   {result.get('content')[:200]}...\\n\\n\"\n",
    "            return result_text\n",
    "        except Exception as e:\n",
    "            return f\"Error during web search: {str(e)}\"\n",
    "\n",
    "    def process_wikipedia(self, title_or_url: str) -> str:\n",
    "        \"\"\"Process a Wikipedia article and store it in the vector database\"\"\"\n",
    "        try:\n",
    "            if title_or_url.startswith(\"http\"):\n",
    "                title = title_or_url.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "            else:\n",
    "                title = title_or_url\n",
    "\n",
    "            wikipedia.set_lang(\"en\")  \n",
    "            try:\n",
    "                page = wikipedia.page(title)\n",
    "            except wikipedia.DisambiguationError as e:\n",
    "                page = wikipedia.page(e.options[0])\n",
    "\n",
    "            content = page.content\n",
    "            chunks = text_splitter.split_text(content)\n",
    "\n",
    "            from langchain_core.documents import Document\n",
    "            documents = [Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"source\": page.url, \"title\": page.title, \"type\": \"wikipedia\"}\n",
    "            ) for chunk in chunks]\n",
    "\n",
    "            self.vector_store.add_documents(documents)\n",
    "            self.vector_store.persist()\n",
    "\n",
    "            return f\"{len(chunks)} chunks from Wikipedia article '{page.title}' added to the vector database.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error processing Wikipedia article: {str(e)}\"\n",
    "\n",
    "    def search_vector_db(self, query: str) -> str:\n",
    "        \"\"\"Search the vector database for relevant documents\"\"\"\n",
    "        try:\n",
    "            docs = self.vector_store.similarity_search(query, k=4)\n",
    "\n",
    "            if not docs:\n",
    "                return \"No relevant information found in the database.\"\n",
    "\n",
    "            results = []\n",
    "            for i, doc in enumerate(docs):\n",
    "                source_info = f\"Source: {doc.metadata.get('source', 'Unknown')}\"\n",
    "                if 'title' in doc.metadata:\n",
    "                    source_info += f\" (Title: {doc.metadata['title']})\"\n",
    "\n",
    "                results.append(f\"Document {i+1}:\\n{doc.page_content}\\n{source_info}\\n\")\n",
    "\n",
    "            return \"\\n\".join(results)\n",
    "        except Exception as e:\n",
    "            return f\"Error searching the vector database: {str(e)}\"\n",
    "\n",
    "    def refine_query(self, original_query: str) -> str:\n",
    "        \"\"\"Improve the user's search query\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Original Query: \"{original_query}\"\n",
    "        \n",
    "        Please improve this query to be more specific and effective for retrieving relevant information.\n",
    "        \n",
    "        The improved query should:\n",
    "        1. Be more precise and less ambiguous\n",
    "        2. Include relevant keywords\n",
    "        3. Be phrased in a search-friendly way\n",
    "        \n",
    "        Improved Query:\n",
    "        \"\"\"\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = gemini_pro.invoke(messages)\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    def process_user_input(self, user_input: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze user input to determine the appropriate action\n",
    "        \"\"\"\n",
    "        if user_input.lower().endswith('.pdf') or \"pdf\" in user_input.lower() and \"process\" in user_input.lower():\n",
    "            pdf_path = user_input.split()[-1]\n",
    "            if pdf_path.endswith('.pdf'):\n",
    "                return {\"action\": \"process_pdf\", \"path\": pdf_path}\n",
    "\n",
    "        if \"wikipedia\" in user_input.lower():\n",
    "            if \"https://\" in user_input:\n",
    "                words = user_input.split()\n",
    "                for word in words:\n",
    "                    if word.startswith(\"https://\") and \"wikipedia\" in word:\n",
    "                        return {\"action\": \"process_wikipedia\", \"url\": word}\n",
    "            else:\n",
    "                prompt = f\"\"\"\n",
    "                Extract the Wikipedia article title from the following text:\n",
    "                {user_input}\n",
    "                \n",
    "                Only return the title, nothing else:\n",
    "                \"\"\"\n",
    "                messages = [HumanMessage(content=prompt)]\n",
    "                response = gemini_pro.invoke(messages)\n",
    "                title = response.content.strip()\n",
    "                return {\"action\": \"process_wikipedia\", \"url\": title}\n",
    "\n",
    "        return {\"action\": \"question\", \"query\": user_input}\n",
    "\n",
    "    def run(self, query: str, chat_history: List[Dict] = None):\n",
    "        \"\"\"Run the Agentic RAG system to handle a user query\"\"\"\n",
    "        if chat_history is None:\n",
    "            chat_history = []\n",
    "\n",
    "        input_analysis = self.process_user_input(query)\n",
    "\n",
    "        if input_analysis[\"action\"] == \"process_pdf\":\n",
    "            result = self.process_pdf(input_analysis[\"path\"])\n",
    "            return {\"output\": result}\n",
    "\n",
    "        elif input_analysis[\"action\"] == \"process_wikipedia\":\n",
    "            result = self.process_wikipedia(input_analysis[\"url\"])\n",
    "            return {\"output\": result}\n",
    "\n",
    "        else:\n",
    "            formatted_chat_history = \"\"\n",
    "            for message in chat_history:\n",
    "                role = message.get(\"role\", \"\")\n",
    "                content = message.get(\"content\", \"\")\n",
    "                formatted_chat_history += f\"{role.capitalize()}: {content}\\n\"\n",
    "\n",
    "            return self.agent_executor.invoke({\n",
    "                \"input\": query,\n",
    "                \"chat_history\": formatted_chat_history\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"Initializing Advanced Agentic RAG System...\")\n",
    "        rag_system = AdvancedAgenticRAG(db_directory=\"./chroma_db\")\n",
    "        print(\"System initialized successfully!\")\n",
    "\n",
    "        pdf_path = r\"D:\\AI\\AI projects\\main\\OpenAI Agents Practical Guide .pdf\"\n",
    "        print(f\"\\nProcessing PDF: {pdf_path}\")\n",
    "        result = rag_system.process_pdf(pdf_path)\n",
    "        print(result)\n",
    "\n",
    "        query = \"What are the key components of an OpenAI agent system according to the guide?\"\n",
    "        response = rag_system.run(query)\n",
    "        print(\"\\nResponse:\", response[\"output\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
